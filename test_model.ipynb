{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import fasttext\n",
    "from itertools import combinations, permutations\n",
    "from copy import copy\n",
    "import sentencepiece as spm\n",
    "from copy import copy\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_name:str,\n",
    "               lang_pair:tuple=None,\n",
    "               test_data:str=None,\n",
    "               tc:str=None,\n",
    "               tc_m:str=None,\n",
    "               sp:str=None,\n",
    "               sp_m:str=None,\n",
    "               tr_m:str=None,\n",
    "               out:str=None,\n",
    "               checkpoint:int=None,\n",
    "               include_tags=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        test_data: Directory contains\n",
    "        \n",
    "    \"\"\"\n",
    "    dir_name = os.path.basename(os.path.dirname(tr_m))\n",
    "    model = os.path.basename(tr_m)\n",
    "    model_name = f'{dir_name}-{model}'\n",
    "    cd = os.getcwd()\n",
    "    data_path = os.path.abspath(test_data)\n",
    "    if not os.path.exists(out):\n",
    "        os.makedirs(out)\n",
    "    (src_lang, tgt_lang) = lang_pair\n",
    "    pair = r\"{}-{}\".format(src_lang, tgt_lang) \n",
    "    tmp = os.path.join(data_path, f\"tmp-{test_name}-{pair}-{model}\")\n",
    "    output = os.path.join(cd, out, f'{test_name}-{pair}-{model_name}-{str(checkpoint)}.res')\n",
    "    if not os.path.exists(tmp):\n",
    "        os.makedirs(tmp)\n",
    "    else:\n",
    "        files = glob.glob(f'{tmp}/*')\n",
    "        for f in files:\n",
    "            try:\n",
    "                os.unlink(f)\n",
    "            except OSError as e:\n",
    "                print(\"Error: %s : %s\" % (f, e.strerror))\n",
    "    if test_name == 'WMT18':\n",
    "        inp_path = os.path.join(tmp,  f'{pair}.src')\n",
    "        r = !sacrebleu -t wmt18 --language {pair} --echo src > {inp_path}\n",
    "    elif test_name == 'WMT20':\n",
    "        inp_path = f'data/dev/wmt20/{src_lang}-{tgt_lang}.{src_lang}'\n",
    "        tgt_path = f'data/dev/wmt20/{src_lang}-{tgt_lang}.{tgt_lang}'     \n",
    "    elif test_name == 'ACCURAT':\n",
    "        inp_path = f'data/test/test.{src_lang}'\n",
    "        tgt_path = f'data/test/test.{tgt_lang}'\n",
    "    else:\n",
    "        print('Unknown test set')\n",
    "    tc_path = os.path.join(cd, tc)\n",
    "    tcm_path = os.path.join(cd, tc_m)\n",
    "    tc_out = os.path.join(tmp, \"tc.out\")\n",
    "    !python {tc_path} {tcm_path} {inp_path} > {tc_out}\n",
    "    !python {sp} --action split  --model {sp_m} --corpora {tc_out}\n",
    "    tags = os.path.join(tmp, pair + '.tag')\n",
    "    sp_name = os.path.basename(sp_m)\n",
    "    sp_out = os.path.join(tmp, sp_name + '-tc.out')\n",
    "    with open(sp_out, 'r') as _input, open(tags, 'w') as tag_output:\n",
    "        for line in _input:\n",
    "            l = len(line.split())\n",
    "            tag_output.write(f\"{' '.join([tgt_lang] * l)}\" + '\\n')\n",
    "    tr_out = os.path.join(tmp, f\"{pair}.out\")\n",
    "    if include_tags:\n",
    "        if checkpoint == 'last':\n",
    "            last_checkpoint = max([int(param.split('.')[-1]) for param in glob.glob(os.path.join(tr_m, 'params.[0-9][0-9][0-9][0-9][0-9]'))])\n",
    "            !python -m sockeye.translate --input-factors {tags} --checkpoints {last_checkpoint} --beam-size 6 --batch-size 100 --disable-device-locking --models {tr_m} --input {sp_out} --output {tr_out}\n",
    "        elif type(checkpoint) == int:\n",
    "            !python -m sockeye.translate --input-factors {tags} --checkpoints {checkpoint} --beam-size 6 --batch-size 100 --disable-device-locking --models {tr_m} --input {sp_out} --output {tr_out}\n",
    "        elif checkpoint == None:\n",
    "            !python -m sockeye.translate --input-factors {tags} --beam-size 6 --batch-size 100 --disable-device-locking --models {tr_m} --input {sp_out} --output {tr_out}\n",
    "    else:\n",
    "        if checkpoint == 'last':\n",
    "            last_checkpoint = max([int(param.split('.')[-1]) for param in glob.glob(os.path.join(tr_m, 'params.[0-9][0-9][0-9][0-9][0-9]'))])\n",
    "            !python -m sockeye.translate --checkpoints {last_checkpoint} --beam-size 6 --batch-size 100 --disable-device-locking --models {tr_m} --input {sp_out} --output {tr_out}\n",
    "        elif type(checkpoint) == int:\n",
    "            !python -m sockeye.translate --checkpoints {checkpoint} --beam-size 6 --batch-size 100 --disable-device-locking --models {tr_m} --input {sp_out} --output {tr_out}\n",
    "        elif checkpoint == None:\n",
    "            !python -m sockeye.translate --beam-size 6 --batch-size 100 --disable-device-locking --models {tr_m} --input {sp_out} --output {tr_out}                                                  \n",
    "    !python {sp} --action restore --corpora {tr_out} --model {sp_m} \n",
    "    res_path = os.path.join(tmp, f'de-{sp_name}-{os.path.basename(tr_out)}')\n",
    "    dir_name = os.path.basename(os.path.dirname(tr_m))\n",
    "    if test_name == 'WMT18':\n",
    "        !cat {res_path} | sacrebleu -t wmt18 -l {pair} > {output}\n",
    "    elif test_name == 'WMT20':\n",
    "        !cat {res_path} | sacrebleu {tgt_path} > {output}                               \n",
    "    elif test_name == 'ACCURAT':\n",
    "        !cat {res_path} | sacrebleu {tgt_path} > {output}\n",
    "    else:\n",
    "        print('Unknown test set')\n",
    "    with open(output) as o:\n",
    "        print(o.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-01 17:19:39.586821: processed 2000 lines\n",
      "02/01/2021 05:19:40 PM INFO: Loading model\n",
      "02/01/2021 05:19:40 PM INFO: Splitting file /gpfs/space/home/kolesnyk/nmt/data/test/baseline-3/tmp-WMT18-en-et-/tc.out\n",
      "[INFO:sockeye.utils] Sockeye version 2.3.2, commit 26c02b1016b0937714ecd4ab367a6a67761ef2df, path /gpfs/space/home/kolesnyk/.conda/envs/gpu_sockeye/lib/python3.7/site-packages/sockeye/__init__.py\n",
      "[INFO:sockeye.utils] MXNet version 1.7.0, path /gpfs/space/home/kolesnyk/.conda/envs/gpu_sockeye/lib/python3.7/site-packages/mxnet/__init__.py\n",
      "[INFO:sockeye.utils] Command: /gpfs/space/home/kolesnyk/.conda/envs/gpu_sockeye/lib/python3.7/site-packages/sockeye/translate.py --checkpoints 182 --beam-size 6 --batch-size 100 --disable-device-locking --models models/baselines/enet_saveoften/ --input /gpfs/space/home/kolesnyk/nmt/data/test/baseline-3/tmp-WMT18-en-et-/sp-tc.out --output /gpfs/space/home/kolesnyk/nmt/data/test/baseline-3/tmp-WMT18-en-et-/en-et.out\n",
      "[INFO:sockeye.utils] Arguments: Namespace(avoid_list=None, batch_size=100, beam_search_stop='all', beam_size=6, brevity_penalty_constant_length_ratio=0.0, brevity_penalty_type='none', brevity_penalty_weight=1.0, bucket_width=10, checkpoints=[182], chunk_size=None, config=None, device_ids=[-1], disable_device_locking=True, dtype=None, ensemble_mode='linear', env=None, input='/gpfs/space/home/kolesnyk/nmt/data/test/baseline-3/tmp-WMT18-en-et-/sp-tc.out', input_factors=None, json_input=False, length_penalty_alpha=1.0, length_penalty_beta=0.0, lock_dir='/tmp', loglevel='INFO', loglevel_secondary_workers='INFO', max_input_length=None, max_output_length=None, max_output_length_num_stds=2, mc_dropout=False, models=['models/baselines/enet_saveoften/'], nbest_size=1, no_hybridization=False, no_logfile=False, omp_num_threads=None, output='/gpfs/space/home/kolesnyk/nmt/data/test/baseline-3/tmp-WMT18-en-et-/en-et.out', output_type='translation', quiet=False, quiet_secondary_workers=False, restrict_lexicon=None, restrict_lexicon_topk=None, sample=None, seed=None, softmax_temperature=None, strip_unknown_words=False, use_cpu=False)\n",
      "[WARNING:sockeye.utils] Sockeye currently does not respect CUDA_VISIBLE_DEVICE settings when locking GPU devices.\n",
      "[INFO:sockeye.utils] Attempting to acquire 1 GPUs of 1 GPUs.\n",
      "[INFO:__main__] Translate Device: gpu(0)\n",
      "[INFO:sockeye.model] Loading 1 model(s) from ['models/baselines/enet_saveoften/'] ...\n",
      "[INFO:sockeye.vocab] Vocabulary (32001 words) loaded from \"models/baselines/enet_saveoften/vocab.src.0.json\"\n",
      "[INFO:sockeye.vocab] Vocabulary (32001 words) loaded from \"models/baselines/enet_saveoften/vocab.trg.0.json\"\n",
      "[INFO:sockeye.model] Model version: 2.3.2\n",
      "[INFO:sockeye.model] Loaded model config from \"models/baselines/enet_saveoften/config\"\n",
      "[INFO:sockeye.model] Disabling dropout layers for performance reasons\n",
      "[INFO:sockeye.model] Config[config_data=Config[data_statistics=Config[average_len_target_per_bucket=[6.545839589998044, 12.341451834812004, 19.854190998499437, 27.64922271407166, 35.55120610686994, 43.50559001380488, 51.43296365914757, 59.40112184736311, 67.3132811472173, 75.12872632336045, 83.1104023347924, 91.00240719624945], buckets=[(8, 8), (16, 16), (24, 24), (32, 32), (40, 40), (48, 48), (56, 56), (64, 64), (72, 72), (80, 80), (88, 88), (96, 96)], length_ratio_mean=1.1535603209542704, length_ratio_stats_per_bucket=[(1.1307886584156304, 0.2723610371314015), (1.1571236715272262, 0.29369800495499115), (1.1402259865969642, 0.24220351506500862), (1.1452343863826848, 0.2191729983829992), (1.1530969054933724, 0.20679820107501148), (1.1645015735750477, 0.20323920266974707), (1.1724642626667632, 0.2042532737661578), (1.183102100003627, 0.2098507514758893), (1.1916570937044368, 0.2198474610933688), (1.1965000506493941, 0.2280798897680178), (1.2074490628048413, 0.23417130364919794), (1.2099159268060693, 0.23807681260973096)], length_ratio_std=0.2403372657202979, max_observed_len_source=96, max_observed_len_target=96, num_discarded=37716, num_sents=3026378, num_sents_per_bucket=[122536, 598181, 697817, 562277, 393000, 257155, 159600, 97696, 60808, 37537, 23985, 15786], num_tokens_source=77097840, num_tokens_target=87100154, num_unks_source=0, num_unks_target=89, size_vocab_source=32001, size_vocab_target=32001], max_seq_len_source=96, max_seq_len_target=96, num_source_factors=1, num_target_factors=1], config_decoder=Config[act_type=relu, attention_heads=8, decoder_type=transformer, depth_key_value=512, dropout_act=0.0, dropout_attention=0.0, dropout_prepost=0.0, feed_forward_num_hidden=2048, lhuc=False, max_seq_len_source=96, max_seq_len_target=96, model_size=512, num_layers=6, positional_embedding_type=fixed, postprocess_sequence=dr, preprocess_sequence=n, use_lhuc=False], config_embed_source=Config[allow_sparse_grad=True, dropout=0.0, factor_configs=None, num_embed=512, num_factors=1, vocab_size=32001], config_embed_target=Config[allow_sparse_grad=True, dropout=0.0, factor_configs=None, num_embed=512, num_factors=1, vocab_size=32001], config_encoder=Config[act_type=relu, attention_heads=8, decoder_type=transformer, depth_key_value=0, dropout_act=0.0, dropout_attention=0.0, dropout_prepost=0.0, feed_forward_num_hidden=2048, lhuc=False, max_seq_len_source=96, max_seq_len_target=96, model_size=512, num_layers=6, positional_embedding_type=fixed, postprocess_sequence=dr, preprocess_sequence=n, use_lhuc=False], config_length_task=None, dtype=float32, intgemm_custom_lib=/gpfs/space/home/kolesnyk/.conda/envs/gpu_sockeye/lib/python3.7/site-packages/sockeye/libintgemm.so, lhuc=False, vocab_source_size=32001, vocab_target_size=32001, weight_tying_type=src_trg_softmax]\n",
      "[INFO:sockeye.model] Model dtype: float32\n",
      "[INFO:sockeye.model] Loaded params from \"models/baselines/enet_saveoften/params.00182\" to \"gpu(0)\"\n",
      "[INFO:sockeye.model] 1 model(s) loaded in 4.7848s\n",
      "[INFO:sockeye.inference] Translator (1 model(s) beam_size=6 beam_search_stop=all max_input_length=95 nbest_size=1 ensemble_mode=None max_batch_size=100 avoiding=0 dtype=float32 softmax_temperature=None)\n",
      "[INFO:__main__] Translating...\n",
      "/gpfs/space/home/kolesnyk/.conda/envs/gpu_sockeye/lib/python3.7/site-packages/sockeye/decoder.py:258: UserWarning: Parameter decoder_transformer_0_att_enc_kv2h_weight, decoder_transformer_3_att_enc_kv2h_weight, decoder_transformer_1_att_enc_kv2h_weight, decoder_transformer_2_att_enc_kv2h_weight, decoder_transformer_5_att_enc_kv2h_weight, decoder_transformer_4_att_enc_kv2h_weight is not used by any computation. Is this intended?\n",
      "  target, autoregr_states = super().forward(step_input, states)\n",
      "[17:19:51] src/imperative/./cached_op.h:257: Disabling fusion due to altered topological order of inputs.\n",
      "[INFO:__main__] Processed 2000 lines. Total time: 41.4478, sec/sent: 0.0207, sent/sec: 48.2534\n",
      "02/01/2021 05:20:34 PM INFO: Loading model\n",
      "02/01/2021 05:20:34 PM INFO: De-sp file /gpfs/space/home/kolesnyk/nmt/data/test/baseline-3/tmp-WMT18-en-et-/en-et.out\n",
      "BLEU+case.mixed+lang.en-et+numrefs.1+smooth.exp+test.wmt18+tok.13a+version.1.4.10 = 17.1 50.4/21.8/11.7/6.8 (BP = 0.997 ratio = 0.997 hyp_len = 36176 ref_len = 36269)\n",
      "\n",
      "CPU times: user 878 ms, sys: 142 ms, total: 1.02 s\n",
      "Wall time: 59.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_model('WMT18',\n",
    "           ('en','et'),\n",
    "           'data/test/baseline-3/',\n",
    "           'scripts/truecaser/applytc.py',\n",
    "           'models/preproc-models/tc-en',\n",
    "           'scripts/word-pieces.py',\n",
    "           'models/preproc-models/sp',\n",
    "           'models/baselines/enet_saveoften/',\n",
    "           'data/test/baseline-3/out',\n",
    "           182,\n",
    "          False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for test in ['WMT20', 'WMT18', 'ACCURAT']:\n",
    "    if test == 'WMT18':\n",
    "        pairs = [ ('en', 'et'), ('et', 'en')]\n",
    "    elif test == 'WMT20'\n",
    "        pairs = [('en', 'ru'), ('ru', 'en')]\n",
    "    elif test == 'ACCURAT':\n",
    "        pairs = [('ru', 'et'), ('et', 'ru')]\n",
    "    print(pairs)\n",
    "    for (src, tgt) in pairs:\n",
    "        for model in glob.glob('models/backtranslate/*[2-8]m_[0-1]'):\n",
    "            name = os.path.basename(model)\n",
    "            temp_dir = os.path.join('data/test/bt-1', name)\n",
    "            out_dir = os.path.join('data/test/bt-1/out', name)\n",
    "            tc_model = \"models/preproc-models/tc\" f'-{src}'\n",
    "            test_model(test,\n",
    "                       (src, tgt),\n",
    "                       temp_dir,\n",
    "                       'scripts/truecaser/applytc.py',\n",
    "                       tc_model,\n",
    "                       'scripts/word-pieces.py',\n",
    "                       'models/preproc-models/sp',\n",
    "                       model,\n",
    "                       out_dir,\n",
    "                      None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in ['WMT20', 'WMT18']:\n",
    "    if test == 'WMT18':\n",
    "        pairs = [ ('en', 'et'), ('et', 'en')]\n",
    "    elif test == 'WMT20':\n",
    "        pairs = [('en', 'ru'), ('ru', 'en')]\n",
    "    elif test == 'ACCURAT':\n",
    "        pairs = [('ru', 'et'), ('et', 'ru')]\n",
    "    print(pairs)\n",
    "    for (src, tgt) in pairs:\n",
    "        for model in glob.glob('models/baselines/*'):\n",
    "            name = os.path.basename(model)\n",
    "            temp_dir = os.path.join('data/test/baseline-3', name)\n",
    "            out_dir = os.path.join('data/test/baseline-3/out', name)\n",
    "            tc_model = \"models/preproc-models/tc\" f'-{src}'\n",
    "            test_model(test,\n",
    "                       (src, tgt),\n",
    "                       temp_dir,\n",
    "                       'scripts/truecaser/applytc.py',\n",
    "                       tc_model,\n",
    "                       'scripts/word-pieces.py',\n",
    "                       'models/preproc-models/sp',\n",
    "                       model,\n",
    "                       out_dir,\n",
    "                      None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_nmt_test",
   "language": "python",
   "name": "gpu_nmt_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
